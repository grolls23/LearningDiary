---
title: "Classification of Remotely Sensed Data"
author: "Gavin Rolls"
date: "3/05/2024"
---

## Summary

Our focus in Remote Sensing this week was on classifying remotely sensed data. Broadly speaking, this refers to dividing areas of a remotely sensed image into discrete categories, often at the pixel or sub-pixel level. Although we discussed a number of examples of classifying EO data, such as a study which mapped forest cover change and classified areas with respect to their forest loss/gain [@hansenHighResolutionGlobalMaps2013], most of this week's lecture was focused on methods of classification, which I will detail below.

### Classification Trees

Classification trees are a data structure which allows a given data point to be classified based on its attributes. Not unlike a flowchart, each node of the classification tree poses a question about a data point and each linkage between nodes represents an answer to that question. Leaf nodes represent the classifications into which a given data point is being sorted

![A diagram illustrating an example of a classification tree - @ibmWhatDecisionTree](img/Decision-Tree.png){width="600"}

What distinguishes a classification tree from a flowchart is that there exists a well-defined method to establish their structure, based on a set of predictor variables, or attributes of each data point. From the root node, data is split on a given attribute (and at a given value if the data is continuous) which minimizes the impurity of the resulting set of points at each child node. This process is then repeated at each node to break the data into more granular categories [@lohClassificationRegressionTrees2011].

Impurity in this context refers to the homogeneity of the sets resulting from a split, with respect to the levels of their other predictor variables [@laber2019]. As a lose example, if splitting a dataset depending on if its attribute $X_1$ is greater or less than five, yields datasets which have a higher 'purity', or similarity than whether or not boolean attribute $X_2$ is true or false, then the node splitting $X_1$ will be farther up the tree.

### Regression Trees

Regression trees are like classification trees, but with the key difference that the resulting data does not fall into classifications, or bins, but instead reflect continuous output variables [@lohClassificationRegressionTrees2011]. This is helpful for situations where the relationship between variables be locally linear but is broadly disjoint across the range of the predictor variable. A regression tree, then, allows data to be split repeatedly at the point at which the subsets generated have the lowest sum of squared residuals (SSR).

#### Side-note: Overfitting

It's possible to create a classification or regression tree that subsets data over and over until each category represents a single data point or a handful of extremely homogeneous data points. Such a structure over-fits data based on this specific input dataset and is unlikely to be more generalisable. Two methods to limit this include stopping the further sub-setting of one's classification tree at a set number of nodes, or by using a method known as weakest link pruning. Without diving into the particular details of the method, its goal is to identify and remove the weakest nodes, that is, nodes which contribute the least to separating data points out into distinct classes.

### Random Forests

A random forest is an extension of the classification and regression tree methods, but relies on a large number of randomly generated trees, as opposed to a single decision tree [@biau2016]. Trees are randomised because, while constructed according to the rule-sets defined above, each tree is built according to a random subset of data points as well as a subset of data point features (attributes). Additionally, data-points can be used in multiple trees in the forest as well as multiple times within a single tree [@biau2016]. The final classification is determined by the category which the highest quantity of trees in the forest assigned to the particular data point.

![An example classification generated by a set of trees in a random forest - @shafi2023](img/Random_Forest.png){width="700"}

The broad motivation for random forests are that they reduce the risk of over-fitting by averaging the predictions from a number of uncorrelated models. Additionally, the random forest model often outperforms single decision trees [@biau2016].

### Support Vector Machine (SVM)

In a departure from the decision tree paradigm, support vector machines utilise not a tree structure to classify objects but instead divide options within the feature space they occupy [@pisner2020]. In a two-dimensional feature space, this is not unlike linear regression wherein a line is drawn in the two-dimensional plane which separates a set of observations into categories based on their $x$ and $y$ values. That said, it does not achieve this by minimising the sum of squared residuals. Instead, it identifies the line that maximises the margin (which can be thought of as empty space) between the classes it separates. The points on the boundary of this margin are referred to as 'support vectors'. Support Vector Machines can also be designed to allow a small number of data points to be misclassified. Although counter-intuitive, allowing errors can make one's model more generalisable [@pisner2020].

![An example of a support vector margin separating data into two classifications - @mathworks](img/SVM.jpg){width="600"}

## Applications in Research

The importance of classification in remote sensing makes it a near ubiquitous phenomena in papers using remote sensing methods. With that said, I'd like to focus on \[PAPERS HERE\].

## Reflection

\[Text\]
