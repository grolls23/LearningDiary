[
  {
    "objectID": "wk1.html#summary",
    "href": "wk1.html#summary",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.1 Summary",
    "text": "1.1 Summary\nRemote sensing refers to methods of data collection whereby phenomena are observed and captured from afar. In the context of geospatial analysis, remote sensing refers to the capture and analysis of data pertaining to Earth’s surface from airborne and particularly spaceborne sensors.\n\n1.1.1 Sensor technologies and data collection\nThe medium of remote sensing is electromagnetic radiation. Remote sensing platforms monitor and record electromagnetic radiation reflected from the earth’s surface at specific wavelengths, including but not limited to the wavelengths of EMR which humans perceive as visible light.\n\n\n\nThe Electromagnetic Spectrum - The Engineering Toolbox (n.d.)\n\n\n\n\n1.1.2 Active versus passive sensors\nSatellite sensors can be classified either as passive sensors or active sensors. A passive sensor only detects radiation reflected from the earth to the sensor and emits no energy of its own. For instance, an optical camera only captures light as it enters the aperture, emitting no energy of its own. An active sensor also captures energy from the surface, but the sensor is also the original source of that energy. For instance, a radar satellite emit radar pulses to the surface to capture their reflection (Earth Science Data Systems 2020).\n\n\n1.1.3 Types of Data Resolution\nRemote sensing platforms attempt to balance several key types of resolution to maximise the breadth and utility of the data collected.\nSpatial Resolution: Remote sensing data comes in the form of raster data, whereby the Earth’s surface is broken up into grid cells. Spatial resolution refers to the size of each of these grid cells. Sensors with higher spatial resolutions can measure EMR at a more granular level, i.e. in smaller grid cells.\nTemporal Resolution: Satellites travel over the Earth’s surface to collect data in regular and consistent orbits and revisit places at well-defined intervals. Satellites that capture more of Earth’s surface at a time (usually as a trade off with spatial resolution) rephotograph given points more frequently leading to a higher number of discrete observations over time.\nSpectral Resolution: EO Satellites monitor specific bands of the EM spectrum, that is, specific wavelengths of radiation. Satellites that monitor more bands at once are said to have higher spectral resolution.\nRadiometric Resolution: Sensors with high radiometric resolution can capture smaller differences in the intensity of radiation being reflected from Earth’s surface. To that end, they will report intensity results with higher precision.\n\n\n1.1.4 Atmospheric Interference\nA challenge of EO is ensuring that radiation reflected to the sensor accurately captures conditions at the surface. Radiation can be distorted in several ways such that data provides an inaccurate account of surface phenomena or is incapable capturing data at all. The most obvious interference is cloud cover, which blocks satellites from recording visual light from the surface.\nOther atmospheric phenomena distort readings, even when radiation can still travel from the surface to the satellite. For instance, the angle at which energy is reflected off the earth and the smoothness of the planet’s surface both cause light to reflect upward at different intensities in different directions. Additionally, light from the sun that never reaches Earth’s surface may instead by reflected off the atmosphere directly and be recorded by the sensor.\n\n\n1.1.5 End user data analysis\nThis week also briefly covered how end users of EO data can download and begin to manipulate remotely sensed data. Several platforms exist to process this sort of data, which is often large and computationally intensive to store and process. We learned how to download data from popular sensing platforms, namely ESA’s Sentinel and USGS’s Landsat. We also learned how to visualise and manipulate data bands in QGIS, and were introduced to SNAP, an application specifically designed to process Sentinel Data."
  },
  {
    "objectID": "wk1.html#applications-in-research",
    "href": "wk1.html#applications-in-research",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.2 Applications in Research",
    "text": "1.2 Applications in Research\nThe field of remote sensing is vast, but one area in which I am particularly interested is the application of light emissions data from the DMSP and VIIRS satellite platforms to map the extent of urban development in polycentric urban entities known as megaregions. Originally pioneered by Richard Florida in his 2008 paper The Rise of the Mega-region (Florida, Gulden, and Mellander 2008), this method uses light emissions as a proxy for urban development to create a more data-driven method of identifying urban extents than municipal boundaries and is not as sensitive to how land cover is classified, as is the case with more traditional methods of identifying urban development from space (Woodall et al. 2023).\nMy particular interest in this research comes from a project I worked on during my undergraduate years where my research group identified satellite-driven approaches to defining megaregions as one of the fundamental methods in past literature and argued for an approach to defining urban boundaries by integrating VIIRS data with landcover data from Landsat and other remotely sensed built environment indicators, in a manner similar to Georg et al. in their analysis of the Boston-Washington corridor (Georg, Blaschke, and Taubenböck 2018).\n\n\n\nSuomi NPP Satellite with VIIRS Sensor - NASA Pace Gallery (n.d.)"
  },
  {
    "objectID": "wk1.html#reflection",
    "href": "wk1.html#reflection",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\nMy favourite part of this week’s class was learning about the motivations for and the trade-offs between different remote sensing platforms. Much of my experience with remote sensing has been in the context of a specific singular remotely sensed dataset, so I have not had the opportunity to compare different platforms (e.g. Landsat vs Sentinel) in the past or consider their respective resolutions.\nAdditionally, my prior exposure to remote sensing has been contained almost entirely to Google Earth Engine which has been implied in this first week of class to have unique advantages when it comes to data management and processing of EO data. I feel as if my experience in the first practical has validated that outlook to some degree, as I found downloading Landsat and Sentinel data and storing it on my local machine to be extremely cumbersome and computationally slow.\nProprietary platforms like SNAP, the ESA applications platform for Sentinel, introduce another layer of difficulty because of incompatibilities with recent MacOS updates. Although I’m excited to develop a stronger understanding of these tools for remote sensing, particularly R, I am looking forward to the return to Earth Engine."
  },
  {
    "objectID": "wk1.html#references",
    "href": "wk1.html#references",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.4 References",
    "text": "1.4 References\n\n\n\n\nEarth Science Data Systems, NASA. 2020. “What Is Synthetic Aperture Radar? | Earthdata.” https://www.earthdata.nasa.gov/learn/backgrounders/what-is-sar.\n\n\nFlorida, Richard, Tim Gulden, and Charlotte Mellander. 2008. “The Rise of the Mega-Region.” Cambridge Journal of Regions, Economy and Society 1 (3): 459–76. https://doi.org/10.1093/cjres/rsn018.\n\n\nGeorg, Isabel, Thomas Blaschke, and Hannes Taubenböck. 2018. “Are We in Boswash Yet? A Multi-Source Geodata Approach to Spatially Delimit Urban Corridors.” ISPRS International Journal of Geo-Information 7 (1): 15. https://doi.org/10.3390/ijgi7010015.\n\n\nNASA Pace Gallery. n.d. “Visible Infrared Imaging Radiometer Suite (VIIRS).” https://pace.oceansciences.org/gallery_more.cgi?id=148.\n\n\nThe Engineering Toolbox. n.d. “Electromagnetic Spectrum.” https://www.engineeringtoolbox.com/electromagnetic-spectrum-d_1929.html.\n\n\nWoodall, Brian, Mariel Borowitz, Kari Watkins, Maria Costa, Angela Howard, Perrine Kemerait, Michelle Lee, et al. 2023. “The Megaregion  Forms, Functions, and Potential? A Literature Review and Proposal for Advancing Research.” International Journal of Urban Sciences 0 (0): 1–23. https://doi.org/10.1080/12265934.2023.2189156."
  },
  {
    "objectID": "index.html#hello",
    "href": "index.html#hello",
    "title": "Gavin Rolls - Learning Diary for CASA 0023",
    "section": "Hello!",
    "text": "Hello!\nI’m Gavin Rolls. Originally from Denver, Colorado in the United States. I’m currently an MSc Student in Urban Spatial Science studying in the Centre for Advanced Spatial Analytics (CASA) at University College London. Before moving to London this autumn, I did my undergraduate degree in Computer Science at the Georgia Institute of Technology in Atlanta, USA.\n\n\n\nTaken Summer 2023 in Osaka, Japan where I travelled to teach the basics of GIS to undergraduate students from Georgia Tech\n\n\nI’m interested in all things cities with particular interests in transport and urban form. Outside of classes, I’m an avid cyclist and runner. In Summer of 2023 my twin brother and I cycled from Naples, IT to Edinburgh, UK and wrote about the experience on a blog linked here.\nThis page is a home for reflections and presentations I’ll be making over the course of second term for CASA 0023 - Remotely Sensing Cities and Environments. Each week of term will have a page of its own, which can be accessed from the sidebar adjacent."
  },
  {
    "objectID": "wk2.html",
    "href": "wk2.html",
    "title": "2  Xaringan Presentation - VIIRS",
    "section": "",
    "text": "This week’s diary is a Xaringan presentation on the Visual Infrared Imaging Radiometry Suite, (VIIRS), an environment-focused sensor platform operated by NASA and NOAA."
  },
  {
    "objectID": "wk3.html#summary",
    "href": "wk3.html#summary",
    "title": "3  Image Correction & Data Joining",
    "section": "3.1 Summary",
    "text": "3.1 Summary\nThis week, we discussed the pre-processing steps that allow us to turn raw remotely sensed data into a product which we can use for analysis. The processing requirements we described fell into two categories which will be detailed below.\n\n3.1.1 Corrections\nA number of types of distortions can occur which can alter where artefacts on the ground are placed in relation to each other or how radiation measured at the satellite differs from conditions at the surface. The main subcategories of distortions and primary methods to correct for them are briefly described here:\nGeometric Correction: The viewing angle of a satellite to the point on Earth’s surface being photographed can cause distortions in the resulting images, arising from inconsistent perspectives. Ideally, a satellite will be at the local zenith (directly above) of the point on Earth being imaged, but this is not always the case. Correcting for this specific type of distortion is known as orthorectification, where images are corrected to appear as if they were taken from nadir (straight down).\n\n\n\nIllustration of local zenith and satellite viewing angle - Space Aeronomy (2011)\n\n\n\n\n\nCartosat-2 photos of the same hilly landscape taken at different viewing zenith angles - Kumari et al. (2019)\n\n\nSimilar distortions can occur as a result of the Earth’s topography or from the rotation of the earth moving the ground underneath the satellite as it orbits.\nTo apply a geometric correction, reference points (called ground control points) are matched between the collected image and a ‘ground truth’ dataset of the surface. From there, a linear regression model is generated and a geometric transformation applied to each point in the image.\nAtmospheric Correction: Interference from Earth’s atmosphere such as cloud cover atmospheric scattering, can cause radiation reflected from Earth’s surface to be distorted or obscured entirely. Atmospheric corrections can be either relative, in that they normalise pixels relative to other points in the image (e.g. a regression based on pseudo-invariant features, points which are likely to be consistent in their reflective properties), or absolute where atmospheric models are used to correct for specific atmospheric conditions.\n\n\n\nRelative Atmospheric Correction as applied to a true colour composite of Sentinel-2B imagery - NASA (2023)\n\n\nRadiometric Calibration: Although not image ‘correction’, radiometric calibration refers to converting the brightness of specific pixels (stored as a unitless digital number by the satellite) into radiance, which is a specific measure of radiative intensity.\nFortunately for professionals (or hobbyists) using remote sensing imagery, level two products can be downloaded for most sensor platforms. A level two product refers to data that has already been pre-processed to correct for many of the distortions described\n\n\n3.1.2 Data Joining\nBecause remote sensing data is collected and provided as sets of discrete images, it becomes important to join adjacent images together if an area of study falls across that boundary. Typically, remotely sensed images are feathered together, meaning a slight area of overlap is blended together to enable a seamless-looking transition, even between temporally distinct images. Typically, brightness values are normalised across the two images before this feathering occurs.\n\n\n\nIllustration of feathering images together - Li et al. (2020)\n\n\nThere are a number of other image enhancements one can do, including normalising brightness across bands or ‘stretching’ data across the entire range of digital numbers for a particular sensor to improve contrast. In the interest of brevity, I will omit a more detailed description here."
  },
  {
    "objectID": "wk3.html#applications-in-research",
    "href": "wk3.html#applications-in-research",
    "title": "3  Image Correction & Data Joining",
    "section": "3.2 Applications in Research",
    "text": "3.2 Applications in Research\nAlthough much of the processing described above is often more of a prerequisite to remote sensing work than the primary focus. With that said, a particular paper of interest I found used geometric distortions not as an image collection error to be corrected but as an additional source of data that adds height data to image collections of Atlanta, Georgia. (Longbotham et al. 2012). By creating what they refer to as a ‘multiangle image sequence’ of Atlanta Georgia using the WorldView-2 imaging satellite height data can be derived from these multi-image datasets, where only synthetic aperture radar (SAR) or LIDAR sensors have typically been used. From this, they used this height data to improve urban classification of the city by 27%.\nAnother paper I found interesting, Fusion based Seamless Mosaic for Remote Sensing Images, concerns the process of mosaicking, or stitching remotely sensed images together. Although in class we discussed the basic method of ‘feathering’ images together, whereby the region overlapped by the two images is converted into a weighted average of the two (with weights depending on which image each pixel is closer to). This paper proposes a method whereby images are decomposed into low-frequency and high-frequency components and processed independently. (Lu, Li, and Fu 2014) For low-frequency parts of images, which are smooth and continuous in nature, the aforementioned feathering effect is used. With that said, high-frequency components such as edges are preserved using a seam-stitching method that enables important structures to be best maintained while enabling smooth transitions of other features between each image."
  },
  {
    "objectID": "wk3.html#reflection",
    "href": "wk3.html#reflection",
    "title": "3  Image Correction & Data Joining",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\nPulling the curtain back, so to speak, on image processing was an enlightening process because my entire experience with remote sensing to date has dealt with pre-processed products that allowed me, as the end user, to bypass any concerns related to image collection entirely. It’s easy to conceptualise remotely sensed images as perfect and uninterrupted when Earth Engine is the window through which all of this data is viewed. Learning that atmospheric models are used to correct for specific distortions or how complex (and computationally expensive) some methods of mosaicking images can be gives me a greater appreciation for the power of the technology we’re applying when we use Landsat, Sentinel, or other EO data. Although I don’t see myself doing any work on image correction tools in the future, I think the context is critical."
  },
  {
    "objectID": "wk3.html#references",
    "href": "wk3.html#references",
    "title": "3  Image Correction & Data Joining",
    "section": "3.4 References",
    "text": "3.4 References\n\n\n\n\nKumari, G. Meena, T. Radhika, R V G. Anjaneyulu, C Venkateswara Rao, and Vinod M Bothale. 2019. “Off-Nadir Viewing Effects in High Resolution Data.” In 2019 IEEE Recent Advances in Geoscience and Remote Sensing : Technologies, Standards and Applications (TENGARSS), 123–27. https://doi.org/10.1109/TENGARSS48957.2019.8976056.\n\n\nLi, Xinghua, Zhiwei Li, Ruitao Feng, Shuang Luo, Chi Zhang, Menghui Jiang, and Huanfeng Shen. 2020. “Generating High-Quality and High-Resolution Seamless Satellite Imagery for Large-Scale Urban Regions.” Remote Sensing 12 (1): 81. https://doi.org/10.3390/rs12010081.\n\n\nLongbotham, Nathan, Chuck Chaapel, Laurence Bleiler, Chris Padwick, William J. Emery, and Fabio Pacifici. 2012. “Very High Resolution Multiangle Urban Classification Analysis.” IEEE Transactions on Geoscience and Remote Sensing 50 (4): 1155–70. https://doi.org/10.1109/TGRS.2011.2165548.\n\n\nLu, Ting, Shutao Li, and Wei Fu. 2014. “Fusion Based Seamless Mosaic for Remote Sensing Images.” Sensing and Imaging 15 (1): 101. https://doi.org/10.1007/s11220-014-0101-0.\n\n\nNASA. 2023. “Atmospheric Correction – Harmonized Landsat Sentinel-2.” August 30, 2023. https://hls.gsfc.nasa.gov/algorithms/atmospheric-correction/.\n\n\nSpace Aeronomy, Royal Belgian Institute for. 2011. “Solar Zenith Angle.” March 1, 2011. https://sacs.aeronomie.be/info/sza.php."
  }
]