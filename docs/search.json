[
  {
    "objectID": "wk1.html#summary",
    "href": "wk1.html#summary",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.1 Summary",
    "text": "1.1 Summary\nRemote sensing refers to methods of data collection whereby phenomena are observed and captured from afar. In the context of geospatial analysis, remote sensing refers to the capture and analysis of data pertaining to Earth’s surface from airborne and particularly spaceborne sensors.\n\n1.1.1 Sensor technologies and data collection\nThe medium of remote sensing is electromagnetic radiation. Remote sensing platforms monitor and record electromagnetic radiation reflected from the earth’s surface at specific wavelengths, including but not limited to the wavelengths of EMR which humans perceive as visible light.\n\n\n\nThe Electromagnetic Spectrum - The Engineering Toolbox (n.d.)\n\n\n\n\n1.1.2 Active versus passive sensors\nSatellite sensors can be classified either as passive sensors or active sensors. A passive sensor only detects radiation reflected from the earth to the sensor and emits no energy of its own. For instance, an optical camera only captures light as it enters the aperture, emitting no energy of its own. An active sensor also captures energy from the surface, but the sensor is also the original source of that energy. For instance, a radar satellite emits radar pulses to the surface to capture their reflection (Earth Science Data Systems 2020).\n\n\n1.1.3 Types of Data Resolution\nRemote sensing platforms attempt to balance several key types of resolution to maximise the breadth and utility of the data collected.\nSpatial Resolution: Remote sensing data comes in the form of raster data, whereby the Earth’s surface is broken up into grid cells. Spatial resolution refers to the size of each of these grid cells. Sensors with higher spatial resolutions can measure EMR at a more granular level, i.e. in smaller grid cells.\nTemporal Resolution: Satellites travel over the Earth’s surface to collect data in regular and consistent orbits and revisit places at well-defined intervals. Satellites that capture more of Earth’s surface at a time (usually as a trade off with spatial resolution) rephotograph given points more frequently leading to a higher number of discrete observations over time.\nSpectral Resolution: EO Satellites monitor specific bands of the EM spectrum, that is, specific wavelengths of radiation. Satellites that monitor more bands at once are said to have higher spectral resolution.\nRadiometric Resolution: Sensors with high radiometric resolution can capture smaller differences in the intensity of radiation being reflected from Earth’s surface. To that end, they will report intensity results with higher precision.\n\n\n1.1.4 Atmospheric Interference\nA challenge of EO is ensuring that radiation reflected to the sensor accurately captures conditions at the surface. Radiation can be distorted in several ways such that data provides an inaccurate account of surface phenomena or is incapable capturing data at all. The most obvious interference is cloud cover, which blocks satellites from recording visual light from the surface.\nOther atmospheric phenomena distort readings, even when radiation can still travel from the surface to the satellite. For instance, the angle at which energy is reflected off the earth and the smoothness of the planet’s surface both cause light to reflect upward at different intensities in different directions. Additionally, light from the sun that never reaches Earth’s surface may instead by reflected off the atmosphere directly and be recorded by the sensor.\n\n\n1.1.5 End user data analysis\nThis week also briefly covered how end users of EO data can download and begin to manipulate remotely sensed data. Several platforms exist to process this sort of data, which is often large and computationally intensive to store and process. We learned how to download data from popular sensing platforms, namely ESA’s Sentinel and USGS’s Landsat. We also learned how to visualise and manipulate data bands in QGIS, and were introduced to SNAP, an application specifically designed to process Sentinel Data."
  },
  {
    "objectID": "wk1.html#applications-in-research",
    "href": "wk1.html#applications-in-research",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.2 Applications in Research",
    "text": "1.2 Applications in Research\nThe field of remote sensing is vast, but one area in which I am particularly interested is the application of light emissions data from the DMSP and VIIRS satellite platforms to map the extent of urban development in polycentric urban entities known as megaregions. Originally pioneered by Richard Florida in his 2008 paper The Rise of the Mega-region (Florida, Gulden, and Mellander 2008), this method uses light emissions as a proxy for urban development to create a more data-driven method of identifying urban extents than municipal boundaries and is not as sensitive to how land cover is classified, as is the case with more traditional methods of identifying urban development from space (Woodall et al. 2023).\nMy particular interest in this research comes from a project I worked on during my undergraduate years where my research group identified satellite-driven approaches to defining megaregions as one of the fundamental methods in past literature and argued for an approach to defining urban boundaries by integrating VIIRS data with landcover data from Landsat and other remotely sensed built environment indicators, in a manner similar to Georg et al. in their analysis of the Boston-Washington corridor (Georg, Blaschke, and Taubenböck 2018).\n\n\n\nSuomi NPP Satellite with VIIRS Sensor - NASA Pace Gallery (n.d.)"
  },
  {
    "objectID": "wk1.html#reflection",
    "href": "wk1.html#reflection",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\nMy favourite part of this week’s class was learning about the motivations for and the trade-offs between different remote sensing platforms. Much of my experience with remote sensing has been in the context of a specific singular remotely sensed dataset, so I have not had the opportunity to compare different platforms (e.g. Landsat vs Sentinel) in the past or consider their respective resolutions.\nAdditionally, my prior exposure to remote sensing has been contained almost entirely to Google Earth Engine which has been implied in this first week of class to have unique advantages when it comes to data management and processing of EO data. I feel as if my experience in the first practical has validated that outlook to some degree, as I found downloading Landsat and Sentinel data and storing it on my local machine to be extremely cumbersome and computationally slow.\nProprietary platforms like SNAP, the ESA applications platform for Sentinel, introduce another layer of difficulty because of incompatibilities with recent MacOS updates. Although I’m excited to develop a stronger understanding of these tools for remote sensing, particularly R, I am looking forward to the return to Earth Engine."
  },
  {
    "objectID": "wk1.html#references",
    "href": "wk1.html#references",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.4 References",
    "text": "1.4 References\n\n\n\n\nEarth Science Data Systems, NASA. 2020. “What Is Synthetic Aperture Radar? | Earthdata.” https://www.earthdata.nasa.gov/learn/backgrounders/what-is-sar.\n\n\nFlorida, Richard, Tim Gulden, and Charlotte Mellander. 2008. “The Rise of the Mega-Region.” Cambridge Journal of Regions, Economy and Society 1 (3): 459–76. https://doi.org/10.1093/cjres/rsn018.\n\n\nGeorg, Isabel, Thomas Blaschke, and Hannes Taubenböck. 2018. “Are We in Boswash Yet? A Multi-Source Geodata Approach to Spatially Delimit Urban Corridors.” ISPRS International Journal of Geo-Information 7 (1): 15. https://doi.org/10.3390/ijgi7010015.\n\n\nNASA Pace Gallery. n.d. “Visible Infrared Imaging Radiometer Suite (VIIRS).” https://pace.oceansciences.org/gallery_more.cgi?id=148.\n\n\nThe Engineering Toolbox. n.d. “Electromagnetic Spectrum.” https://www.engineeringtoolbox.com/electromagnetic-spectrum-d_1929.html.\n\n\nWoodall, Brian, Mariel Borowitz, Kari Watkins, Maria Costa, Angela Howard, Perrine Kemerait, Michelle Lee, et al. 2023. “The Megaregion  Forms, Functions, and Potential? A Literature Review and Proposal for Advancing Research.” International Journal of Urban Sciences 0 (0): 1–23. https://doi.org/10.1080/12265934.2023.2189156."
  },
  {
    "objectID": "index.html#hello",
    "href": "index.html#hello",
    "title": "Gavin Rolls - Learning Diary for CASA 0023",
    "section": "Hello!",
    "text": "Hello!\nI’m Gavin Rolls. Originally from Denver, Colorado in the United States. I’m currently an MSc Student in Urban Spatial Science studying in the Centre for Advanced Spatial Analytics (CASA) at University College London. Before moving to London this autumn, I did my undergraduate degree in Computer Science at the Georgia Institute of Technology in Atlanta, USA.\n\n\n\nTaken Summer 2023 in Osaka, Japan where I travelled to teach the basics of GIS to undergraduate students from Georgia Tech\n\n\nI’m interested in all things cities with particular interests in transport and urban form. Outside of classes, I’m an avid cyclist and runner. In Summer of 2023 my twin brother and I cycled from Naples, IT to Edinburgh, UK and wrote about the experience on a blog linked here.\nThis page is a home for reflections and presentations I’ll be making over the course of second term for CASA 0023 - Remotely Sensing Cities and Environments. Each week of term will have a page of its own, which can be accessed from the sidebar adjacent."
  },
  {
    "objectID": "wk2.html",
    "href": "wk2.html",
    "title": "2  Xaringan Presentation - VIIRS",
    "section": "",
    "text": "This week’s diary is a Xaringan presentation on the Visual Infrared Imaging Radiometry Suite, (VIIRS), an environment-focused sensor platform operated by NASA and NOAA."
  },
  {
    "objectID": "wk3.html#summary",
    "href": "wk3.html#summary",
    "title": "3  Image Correction & Data Joining",
    "section": "3.1 Summary",
    "text": "3.1 Summary\nThis week, we discussed the pre-processing steps that allow us to turn raw remotely sensed data into a product which we can use for analysis. The processing requirements we described fell into two categories which will be detailed below.\n\n3.1.1 Corrections\nA number of types of distortions can occur which can alter where artefacts on the ground are placed in relation to each other or how radiation measured at the satellite differs from conditions at the surface. The main subcategories of distortions and primary methods to correct for them are briefly described here:\nGeometric Correction: The viewing angle of a satellite to the point on Earth’s surface being photographed can cause distortions in the resulting images, arising from inconsistent perspectives. Ideally, a satellite will be at the local zenith (directly above) of the point on Earth being imaged, but this is not always the case. Correcting for this specific type of distortion is known as orthorectification, where images are corrected to appear as if they were taken from nadir (straight down).\n\n\n\nIllustration of local zenith and satellite viewing angle - Space Aeronomy (2011)\n\n\n\n\n\nCartosat-2 photos of the same hilly landscape taken at different viewing zenith angles - Kumari et al. (2019)\n\n\nSimilar distortions can occur as a result of the Earth’s topography or from the rotation of the earth moving the ground underneath the satellite as it orbits.\nTo apply a geometric correction, reference points (called ground control points) are matched between the collected image and a ‘ground truth’ dataset of the surface. From there, a linear regression model is generated and a geometric transformation applied to each point in the image.\nAtmospheric Correction: Interference from Earth’s atmosphere such as cloud cover atmospheric scattering, can cause radiation reflected from Earth’s surface to be distorted or obscured entirely. Atmospheric corrections can be either relative, in that they normalise pixels relative to other points in the image (e.g. a regression based on pseudo-invariant features, points which are likely to be consistent in their reflective properties), or absolute where atmospheric models are used to correct for specific atmospheric conditions.\n\n\n\nRelative Atmospheric Correction as applied to a true colour composite of Sentinel-2B imagery - NASA (2023)\n\n\nRadiometric Calibration: Although not image ‘correction’, radiometric calibration refers to converting the brightness of specific pixels (stored as a unitless digital number by the satellite) into radiance, which is a specific measure of radiative intensity.\nFortunately for professionals (or hobbyists) using remote sensing imagery, level two products can be downloaded for most sensor platforms. A level two product refers to data that has already been pre-processed to correct for many of the distortions described.\n\n\n3.1.2 Data Joining\nBecause remote sensing data is collected and provided as sets of discrete images, it becomes important to join adjacent images together if an area of study falls across that boundary. Typically, remotely sensed images are feathered together, meaning a slight area of overlap is blended together to enable a seamless-looking transition, even between temporally distinct images. Typically, brightness values are normalised across the two images before this feathering occurs.\n\n\n\nIllustration of feathering images together - Li et al. (2020)\n\n\nThere are a number of other image enhancements one can do, including normalising brightness across bands or ‘stretching’ data across the entire range of digital numbers for a particular sensor to improve contrast. In the interest of brevity, I will omit a more detailed description here."
  },
  {
    "objectID": "wk3.html#applications-in-research",
    "href": "wk3.html#applications-in-research",
    "title": "3  Image Correction & Data Joining",
    "section": "3.2 Applications in Research",
    "text": "3.2 Applications in Research\nMuch of the processing described above is often more of a prerequisite to remote sensing work than the primary focus. With that said, a particular paper of interest I found used geometric distortions not as an image collection error to be corrected but as an additional source of data that adds height information to image collections of Atlanta, Georgia (Longbotham et al. 2012). By creating what they refer to as a ‘multiangle image sequence’ of Atlanta Georgia using the WorldView-2 imaging satellite height data can be derived from these multi-image datasets, where only synthetic aperture radar (SAR) or LIDAR sensors have typically been used. From this, they used this height data to improve urban classification of the city by 27%.\nAnother paper I found interesting, Fusion based Seamless Mosaic for Remote Sensing Images, concerns the process of mosaicking, or stitching remotely sensed images together. In class we discussed the basic method of ‘feathering’ images together, whereby the region overlapped by the two images is converted into a weighted average of the two (with weights depending on which image each pixel is closer to). This paper proposes a method whereby images are decomposed into low-frequency and high-frequency components and processed independently. (Lu, Li, and Fu 2014) For low-frequency parts of images, which are smooth and continuous in nature, the aforementioned feathering effect is used. With that said, high-frequency components such as edges are preserved using a seam-stitching method that enables important structures to be best maintained while enabling smooth transitions of other features between each image."
  },
  {
    "objectID": "wk3.html#reflection",
    "href": "wk3.html#reflection",
    "title": "3  Image Correction & Data Joining",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\nPulling the curtain back, so to speak, on image processing was an enlightening process because my entire experience with remote sensing to date has dealt with pre-processed products that allowed me, as the end user, to bypass any concerns related to image collection entirely. It’s easy to conceptualise remotely sensed images as perfect and uninterrupted when Earth Engine is the window through which all of this data is viewed. Learning that atmospheric models are used to correct for specific distortions or how complex (and computationally expensive) some methods of mosaicking images can be gives me a greater appreciation for the power of the technology we’re applying when we use Landsat, Sentinel, or other EO data. Although I don’t see myself doing any work on image correction tools in the future, I think the context is critical."
  },
  {
    "objectID": "wk3.html#references",
    "href": "wk3.html#references",
    "title": "3  Image Correction & Data Joining",
    "section": "3.4 References",
    "text": "3.4 References\n\n\n\n\nKumari, G. Meena, T. Radhika, R V G. Anjaneyulu, C Venkateswara Rao, and Vinod M Bothale. 2019. “Off-Nadir Viewing Effects in High Resolution Data.” In 2019 IEEE Recent Advances in Geoscience and Remote Sensing : Technologies, Standards and Applications (TENGARSS), 123–27. https://doi.org/10.1109/TENGARSS48957.2019.8976056.\n\n\nLi, Xinghua, Zhiwei Li, Ruitao Feng, Shuang Luo, Chi Zhang, Menghui Jiang, and Huanfeng Shen. 2020. “Generating High-Quality and High-Resolution Seamless Satellite Imagery for Large-Scale Urban Regions.” Remote Sensing 12 (1): 81. https://doi.org/10.3390/rs12010081.\n\n\nLongbotham, Nathan, Chuck Chaapel, Laurence Bleiler, Chris Padwick, William J. Emery, and Fabio Pacifici. 2012. “Very High Resolution Multiangle Urban Classification Analysis.” IEEE Transactions on Geoscience and Remote Sensing 50 (4): 1155–70. https://doi.org/10.1109/TGRS.2011.2165548.\n\n\nLu, Ting, Shutao Li, and Wei Fu. 2014. “Fusion Based Seamless Mosaic for Remote Sensing Images.” Sensing and Imaging 15 (1): 101. https://doi.org/10.1007/s11220-014-0101-0.\n\n\nNASA. 2023. “Atmospheric Correction – Harmonized Landsat Sentinel-2.” August 30, 2023. https://hls.gsfc.nasa.gov/algorithms/atmospheric-correction/.\n\n\nSpace Aeronomy, Royal Belgian Institute for. 2011. “Solar Zenith Angle.” March 1, 2011. https://sacs.aeronomie.be/info/sza.php."
  },
  {
    "objectID": "wk4.html#city-of-choice-policy-concern",
    "href": "wk4.html#city-of-choice-policy-concern",
    "title": "4  Remote Sensing for Urban Policy",
    "section": "4.1 City of Choice & Policy Concern",
    "text": "4.1 City of Choice & Policy Concern\nIn Week 4, we’ve been tasked with identifying a particular city and policy challenge it faces which could be investigated or solved with remote sensing.\nI haven’t yet ‘visited’ my hometown of Denver in my degree programme yet and seeing that it deals with a number of problems (many of which I’m particularly passionate about), I wanted to take a look.\nI want to talk not just about my problem of interest, urban heat effects, but also a particular contributing factor - high amounts of impervious surface area and specifically parking lots. Impervious surfaces are a global problem but America’s high automotive dependence and sprawling development patters have resulted in over 110 thousand square kilometres of surface area across the country (Frazer 2005). Most of the literature I’ve come across has talked about impervious surfaces focuses on either flooding (Feng, Zhang, and Bourke 2021) or heat island effects, but I wish to focus on the latter.\n\n\n\nImpervious surfaces in Central Denver, as captured by the IKONOS sensor - Corporation (n.d.)\n\n\nCentral regions of Denver experience a heat island effect relative to its surroundings that ranges anywhere from 3.5 to 4.2C depending on the time of day and current season (Thunen 2013). Cooling the city is a critical goal, and the UN Environment Programme identifies heat mitigation in urban areas as a goal of immense importance in its ‘Beating the Heat’ handbook for cities. (Environment 2021). Denver itself indicates in its climate action plan that the city is likely to experience increasingly intense heat events in coming decades and that improving resilience along this dimension is critical (Denver 2018).\nAlthough it’s not possible to eliminate impervious surfaces entirely, one phenomena I noticed growing up in Denver was the massive number of vacant lots and unused parking lots which would remain unused for years on end. Large lots come with a number of environmental side effects and even when taken alone, contribute meaningfully to heat island effects (Davis et al. 2010). Additionally, research has shown that infill can have a positive effect on heat island effects (Helmholz et al. 2021)."
  },
  {
    "objectID": "wk4.html#remote-sensing-dataset-of-use",
    "href": "wk4.html#remote-sensing-dataset-of-use",
    "title": "4  Remote Sensing for Urban Policy",
    "section": "4.2 Remote Sensing Dataset of Use",
    "text": "4.2 Remote Sensing Dataset of Use\nThe good news is that there exist a number of remotely sensed datasets which are extremely useful at identifying both impervious surface areas as well as the heat effects they generate. I would propose a method that applies the following datasets to be able to effectively identify areas of impervious surfaces, particularly parking lots, that are a) not part of existing structures and b) contributing to local heat effects:\n\n4.2.1 Land Surface Temperature (LST)\nAll active Landsat platforms collect data in the thermal infrared band at 30 metre resolution (Sayler 2023) and its use in studying urban heat island effects is extensive (Xu et al. 2023). In order to investigate urban heating at a higher spatial resolution, this platform is preferable to using meteorological data as has been done in the past in the context of Denver (Thunen 2013).\n\n\n\nLandsat 9 RGB and LST readings compared - Sayler (2023)\n\n\n\n\n4.2.2 Impervious Surface Area (ISA)\nAlthough not a primary Landsat product like LST, Impervious Surface area can be derived from Landsat products and is available to researchers as a remote sensing product (Zhang et al. 2022). This data is also available at 30 metre resolution.\n\n\n4.2.3 OSM Building Footprint Data\nAs impervious surfaces cover a large proportion of land in metropolitan Denver, using OSM Data could help narrow the search to identify disused lots or areas of impervious surfaces which could otherwise be altered.\n\n\n4.2.4 Taken Together…\nIt would be my hope that a combination of these datasets would allow policy-makers to identify the areas of Denver which most suffer from heat island effects and be able to search for nearby lots which might be eligible for infill or conversion into greenspace in order to help alleviate local effects.\nWith that said, impervious surfaces like parking lots are only one piece of the heat island puzzle and attempts to tackle heat islands at a city-level should absolutely apply other methods put forward by the ‘Beating the Heat’ manual such as heat-resilient infrastructure, street trees, and expanded greenspace, to name a few (Environment 2021)."
  },
  {
    "objectID": "wk4.html#advancing-existing-methods",
    "href": "wk4.html#advancing-existing-methods",
    "title": "4  Remote Sensing for Urban Policy",
    "section": "4.3 Advancing Existing Methods",
    "text": "4.3 Advancing Existing Methods\nIn addition to building upon existing methodologically-based heat maps, using remote sensing to mitigate heat islands, especially in the context of impervious surfaces constitutes a shift from the methods most often used to address heat. The policy approaches most often employed include building with high albedo materials, improving ground vegetation and shade trees, and green roofs and facades (Degirmenci et al. 2021). I think remotely sensed data provides an incredible opportunity to address root causes instead of mitigating effects, hence my interest in suggesting that it be used as a tool to identify areas of heat-insensitive development."
  },
  {
    "objectID": "wk4.html#reflection",
    "href": "wk4.html#reflection",
    "title": "4  Remote Sensing for Urban Policy",
    "section": "4.4 Reflection",
    "text": "4.4 Reflection\nI found it more challenging than I anticipated to identify a combination of city, problem, data source, and policy suggestion that fit the bill for this week’s assignment. When looking around at what work had been done, I almost felt as if the whole span of applications had already been covered. In reality, it seems that a lot of approaches which feel like common sense are being ignored by a lot of municipalities. I don’t know if a city like Denver simply doesn’t have the bandwidth to further investigate heat effects, if it’s working hard through other means to fix them, or if it’s not concerned with them at all. Municipal documentation has been quite poor on that front.\nAdditionally, I find it surprising that so much of the work of urban heat resilience feels as if it’s concerned with mitigating the impact of heat than actually trying to lower the temperature. Shade shelters in particular feel like a bizarre first solution because, while I can see why they might be important and necessary in a number of circumstances, they won’t be any good at cooling cities down themselves like improved vegetation cover or reducing impervious surfaces would."
  },
  {
    "objectID": "wk4.html#references",
    "href": "wk4.html#references",
    "title": "4  Remote Sensing for Urban Policy",
    "section": "4.5 References",
    "text": "4.5 References\n\n\n\n\nCorporation, Satellite Imaging. n.d. “IKONOS Satellite Image of Denver, Colorado  Satellite Imaging Corp.” Accessed February 1, 2024. https://www.satimagingcorp.com/gallery/ikonos/ikonos-denver-lg/.\n\n\nDavis, Amélie Y., Bryan C. Pijanowski, Kimberly Robinson, and Bernard Engel. 2010. “The Environmental and Economic Costs of Sprawling Parking Lots in the United States.” Land Use Policy, Forest transitions, 27 (2): 255–61. https://doi.org/10.1016/j.landusepol.2009.03.002.\n\n\nDegirmenci, Kenan, Kevin C. Desouza, Walter Fieuw, Richard T. Watson, and Tan Yigitcanlar. 2021. “Understanding Policy and Technology Responses in Mitigating Urban Heat Islands: A Literature Review and Directions for Future Research.” Sustainable Cities and Society 70 (July): 102873. https://doi.org/10.1016/j.scs.2021.102873.\n\n\nDenver, City of. 2018. “Denver 80 x 50 Climate Action Plan.” https://www.denvergov.org/files/assets/public/v/1/climate-action/documents/ddphe_80x50_climateactionplan.pdf.\n\n\nEnvironment, U. N. 2021. “Beating the Heat: A Sustainable Cooling Handbook for Cities.” http://www.unep.org/resources/report/beating-heat-sustainable-cooling-handbook-cities.\n\n\nFeng, Boyu, Ying Zhang, and Robin Bourke. 2021. “Urbanization Impacts on Flood Risks Based on Urban Growth Data and Coupled Flood Models.” Natural Hazards 106 (1): 613–27. https://doi.org/10.1007/s11069-020-04480-0.\n\n\nFrazer, Lance. 2005. “Paving Paradise: The Peril of Impervious Surfaces.” Environmental Health Perspectives 113 (7): A456–62. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1257665/.\n\n\nHelmholz, P., D. Bulatov, B. Kottler, P. Burton, F. Mancini, M. May, E. Strauß, and M. Hecht. 2021. “Quantifying the Impact of Urban Infull on the Urban Heat Island Effect - a Case Study for an Alternative Medium Density Model.” In The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, XLVI-4-W1-2021:43–50. https://doi.org/10.5194/isprs-archives-XLVI-4-W1-2021-43-2021.\n\n\nSayler, Kristi. 2023. “Landsat 8-9 Collection 2 (C2) Level2 Science Product Guide.” https://www.usgs.gov/landsat-missions/landsat-collection-2-surface-temperature.\n\n\nThunen, Diana. 2013. “The Urban Heat Island and Its Influence on Precipitation in Denver, Colorado.” PhD thesis, Denver University. https://digitalcommons.du.edu/etd/650.\n\n\nXu, Xiong, Haoyang Pei, Chao Wang, Qingyu Xu, Huan Xie, Yanmin Jin, Yongjiu Feng, Xiaohua Tong, and Changjiang Xiao. 2023. “Long-Term Analysis of the Urban Heat Island Effect Using Multisource Landsat Images Considering Inter-Class Differences in Land Surface Temperature Products.” Science of The Total Environment 858 (February): 159777. https://doi.org/10.1016/j.scitotenv.2022.159777.\n\n\nZhang, Xiao, Liangyun Liu, Tingting Zhao, Yuan Gao, Xidong Chen, and Jun Mi. 2022. “GISD30: Global 30&thinsp;m Impervious-Surface Dynamic Dataset from 1985 to 2020 Using Time-Series Landsat Imagery on the Google Earth Engine Platform.” Earth System Science Data 14 (4): 1831–56. https://doi.org/10.5194/essd-14-1831-2022."
  },
  {
    "objectID": "wk5.html#week-five-was-a-week-off---content-will-resume-in-week-six",
    "href": "wk5.html#week-five-was-a-week-off---content-will-resume-in-week-six",
    "title": "5  Week Off",
    "section": "5.1 Week Five was a week off - content will resume in Week Six",
    "text": "5.1 Week Five was a week off - content will resume in Week Six"
  },
  {
    "objectID": "wk5.html",
    "href": "wk5.html",
    "title": "5  Week Off",
    "section": "",
    "text": "Week Five was a week off - content will resume in Week Six."
  },
  {
    "objectID": "wk6.html#summary",
    "href": "wk6.html#summary",
    "title": "6  Introduction to Earth Engine",
    "section": "6.1 Summary",
    "text": "6.1 Summary\nAfter the a week off from class, we were introduced to Google Earth Engine, the platform for remotely sensed data analysis which we will make our home for the remainder of the course. The number of specific tools and operations we learned about in Earth Engine is too great to share concisely in this learning diary format. For that reason, I will focus on what Earth Engine is and what makes it both unique and powerful as a spatial analysis platform.\n\n6.1.1 What is Earth Engine?\nGoogle Earth Engine is a high-performance geospatial analysis tool, with a particular focus on remote sensing, which was developed to allow researchers and other professionals to use high performance computing technologies in their work without requiring the requisite IT skill which has been required to take advantage of these resources in the past (Gorelick et al. 2017). The nature of remotely sensed data - raster datasets covering large expanses of the world’s surface - requires large computing resources to process at scale.\nEarth Engine satisfies these computational requirements by utilising a technique called parallel computing, wherein computationally intensive tasks are split into independent sub-tasks and processed simultaneously on multiple CPUs (Gorelick et al. 2017). The core idea behind the application’s architecture then, is that processing happens server-side instead of on a local machine as GIS programmes have typically required. Users interface with Earth Engine using JavaScript or Python-enabled libraries.\n\n\n\nThe basic Earth Engine architecture - Gorelick et al. (2017)\n\n\nIn addition to reducing the local computational power required to efficiently utilise remotely sensed data, Earth Engine claims its other distinct advantage as that of data access and management. Google provides a ‘data catalogue’ which contains petabytes of the most commonly used (and arguably important) remotely sensed datasets available (Campos-Taberner et al. 2018). In addition to being constantly updated, this catalogue also includes historical readings from many satellite platforms which, when combined with the use of the aforementioned parallel processing paradigm, makes temporal analysis of RS data much easier (Campos-Taberner et al. 2018).\n\n\n\nSamples of Earth Engine’s Data Catalog - Earth Engine (2019)"
  },
  {
    "objectID": "wk6.html#applications-in-research",
    "href": "wk6.html#applications-in-research",
    "title": "6  Introduction to Earth Engine",
    "section": "6.2 Applications in Research",
    "text": "6.2 Applications in Research\nGoogle Earth Engine has seen increasingly wide use in recent years, although the most common characteristics of its applications, per Tamiminia et al. (2020) are:\n\nThe use of multi-temporal data\nUse of data from optical sensor platforms, particularly Landsat and MODIS\nApplications with a particular focus on phenomena in the natural environment\n\nIn particular, products like NDVI, land-cover, and forest change datasets were used in a substantial number of applications. Additionally, the application of machine learning techniques is becoming increasingly common (Tamiminia et al. 2020).\nWith that said, the two applications I’d like to further investigate are the use of Landsat imagery for population mapping and urban change over time from Patel et al. (2015) and monitoring changes in heat island effects as a result of land cover changes from Ravanelli et al. (2018).\nPatel et al. (2015) measure urban change over the 40-year lifespan of the Landsat programme by extracting urban areas from Landsat data. To do this, they calculate the Normalized Difference Spectral Vector (NDSV), which is a vector containing all possible normalised indexes of band pairs for a total of 30. They write that urban areas have a unique NDSV signature and can thus be used to identify urban change. From that point, they apply a random forest model to estimate area populations. As the authors themselves write, Earth Engine is the obvious tool to use in this context as it allows for the efficient processing of multi-temporal RS data, making it perfect for analysing urban change over time.\nRavanelli et al. (2018) also take advantage of the spatio-temporal richness of Earth Engine’s data catalogue, but apply it instead to surface urban heat island effects, or SUHI, in an attempt to understand how land cover changes may play a role. To do this, they use land-cover maps (NCLD, as the study is US centric) to measure land-cover change and Climate Engine, a tool integrated with Earth Engine, to derive land surface temperature (LST) from the Landsat product. They then apply a simple linear regression model on a per-pixel basis to understand the relationship with LST. Ravanelli et al. (2018) were less concerned with specific results and more interested in demonstrating the potential efficacy of their method for connecting LST and land-cover. To that end, they identified inconsistencies in their data between swings in LST versus air temperature as measured from ground-based weather monitoring stations."
  },
  {
    "objectID": "wk6.html#reflection",
    "href": "wk6.html#reflection",
    "title": "6  Introduction to Earth Engine",
    "section": "6.3 Reflection",
    "text": "6.3 Reflection\nAt a personal level, I have an interesting relationship with Google Earth Engine. The platform was my first exposure to geospatial science at large and it catalysed my interest in the topic. With that said, I was brought onto the research group in which I used Earth Engine because of my computer science background and as a result, taught myself much of what I know now. I’m excited, then, to learn about Earth Engine in a formal context and hopefully clarify many of the misunderstandings I inevitably generated on my earlier journey. Because of the fact that I was self-taught, I always found the JavaScript-based approach to GIS to be quite cumbersome and found QGIS much easier to learn and understand. With that said, reading the original papers on the origins of Earth Engine has me far more sold on its merits than I originally was when I started using the system. The computational power it grants users to is mind-boggling, and I am very much being literal when I say that.\nA topic of particular interest to me is that of Earth Engine and its use in machine learning. Machine learning broadly is a topic which I have spent very little time focusing on and until this week’s lecture, I wasn’t aware that it was one of the main selling points of Earth Engine. It’s something I’d like to explore down the line, be in it a dissertation or personal project."
  },
  {
    "objectID": "wk6.html#references",
    "href": "wk6.html#references",
    "title": "6  Introduction to Earth Engine",
    "section": "6.4 References",
    "text": "6.4 References\n\n\n\n\nCampos-Taberner, Manuel, Álvaro Moreno-Martínez, Francisco Javier García-Haro, Gustau Camps-Valls, Nathaniel P. Robinson, Jens Kattge, and Steven W. Running. 2018. “Global Estimation of Biophysical Variables from Google Earth Engine Platform.” Remote Sensing 10 (8): 1167. https://doi.org/10.3390/rs10081167.\n\n\nEarth Engine, Google. 2019. “Our Latest Additions to the Earth Engine Data Catalog.” https://medium.com/google-earth/our-latest-additions-to-the-earth-engine-data-catalog-ded9c563f676.\n\n\nGorelick, Noel, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David Thau, and Rebecca Moore. 2017. “Google Earth Engine: Planetary-Scale Geospatial Analysis for Everyone.” Remote Sensing of Environment, Big Remotely Sensed Data: Tools, applications and experiences, 202 (December): 18–27. https://doi.org/10.1016/j.rse.2017.06.031.\n\n\nPatel, Nirav N., Emanuele Angiuli, Paolo Gamba, Andrea Gaughan, Gianni Lisini, Forrest R. Stevens, Andrew J. Tatem, and Giovanna Trianni. 2015. “Multitemporal Settlement and Population Mapping from Landsat Using Google Earth Engine.” International Journal of Applied Earth Observation and Geoinformation 35 (March): 199–208. https://doi.org/10.1016/j.jag.2014.09.005.\n\n\nRavanelli, Roberta, Andrea Nascetti, Raffaella Valeria Cirigliano, Clarissa Di Rico, Giovanni Leuzzi, Paolo Monti, and Mattia Crespi. 2018. “Monitoring the Impact of Land Cover Change on Surface Urban Heat Island Through Google Earth Engine: Proposal of a Global Methodology, First Applications and Problems.” Remote Sensing 10 (9): 1488. https://doi.org/10.3390/rs10091488.\n\n\nTamiminia, Haifa, Bahram Salehi, Masoud Mahdianpari, Lindi Quackenbush, Sarina Adeli, and Brian Brisco. 2020. “Google Earth Engine for Geo-Big Data Applications: A Meta-Analysis and Systematic Review.” ISPRS Journal of Photogrammetry and Remote Sensing 164 (June): 152–70. https://doi.org/10.1016/j.isprsjprs.2020.04.001."
  },
  {
    "objectID": "wk7.html#summary",
    "href": "wk7.html#summary",
    "title": "7  Classification of Remotely Sensed Data",
    "section": "7.1 Summary",
    "text": "7.1 Summary\nOur focus in Remote Sensing this week was on classifying remotely sensed data. Broadly speaking, this refers to dividing areas of a remotely sensed image into discrete categories, often at the pixel or sub-pixel level. Although we discussed a number of examples of classifying EO data, such as a study which mapped forest cover change and classified areas with respect to their forest loss/gain (Hansen et al. 2013), most of this week’s lecture was focused on methods of classification, which I will detail below.\n\n7.1.1 Classification Trees\nClassification trees are a data structure which allows a given data point to be classified based on its attributes. Not unlike a flowchart, each node of the classification tree poses a question about a data point and each linkage between nodes represents an answer to those questions. Leaf nodes represent the classifications into which a given data point is being sorted.\n\n\n\nA diagram illustrating an example of a classification tree - IBM (n.d.)\n\n\nWhat distinguishes a classification tree from a flowchart is that there exists a well-defined method to establish their structure, based on a set of predictor variables, or attributes of each data point. From the root node, data is split on a given attribute (and at a given value if the data is continuous) which minimizes the impurity of the resulting set of points at each child node. This process is then repeated at each node to break the data into more granular categories (Loh 2011).\nImpurity in this context refers to the homogeneity of the sets resulting from a split, with respect to the levels of their other predictor variables (Laber and Murtinho 2019). As a lose example, if splitting a dataset depending on if its attribute \\(X_1\\) is greater or less than five, yields datasets which have a higher ‘purity’, or similarity than whether or not boolean attribute \\(X_2\\) is true or false, then the node splitting \\(X_1\\) will be closer to the root of the tree.\n\n\n7.1.2 Regression Trees\nRegression trees are like classification trees, but with the key difference that the resulting data does not fall into classifications, or bins, but instead reflect continuous output variables (Loh 2011). This is helpful for situations where the relationship between variables may be locally linear but is broadly disjoint across the range of the predictor variable. A regression tree, then, allows data to be split repeatedly at the point at which the subsets generated have the lowest sum of squared residuals (SSR).\n\n7.1.2.1 Side-note: Overfitting\nIt’s possible to create a classification or regression tree that subsets data over and over until each category represents a single data point or a handful of extremely homogeneous data points. Such a structure over-fits data based on this specific input dataset and is unlikely to be more generalisable. Two methods to limit this include stopping the further sub-setting of one’s classification tree at a set number of nodes, or by using a method known as weakest link pruning. Without diving into the particular details of the method, its goal is to identify and remove the weakest nodes, that is, nodes which contribute the least to separating data points out into distinct classes.\n\n\n\n7.1.3 Random Forests\nA random forest is an extension of the classification and regression tree methods, but relies on a large number of randomly generated trees, as opposed to a single decision tree (Biau and Scornet 2016). Trees are randomised because, while constructed according to the rule-sets defined above, each tree is built according to a random subset of data points as well as a subset of data point features (attributes). Additionally, data-points can be used in multiple trees in the forest as well as multiple times within a single tree (Biau and Scornet 2016). The final classification is determined by the category which the highest quantity of trees in the forest assigned to the particular data point.\n\n\n\nAn example classification generated by a set of trees in a random forest - Shafi (2023)\n\n\nThe broad motivation for random forests are that they reduce the risk of over-fitting by averaging the predictions from a number of uncorrelated models. Additionally, the random forest model often outperforms single decision trees (Biau and Scornet 2016).\n\n\n7.1.4 Support Vector Machine (SVM)\nIn a departure from the decision tree paradigm, support vector machines utilise not a tree structure to classify objects but instead divide options within the feature space they occupy (Pisner and Schnyer 2020). In a two-dimensional feature space, this is not unlike linear regression wherein a boundary is drawn in the two-dimensional plane based on a set of observations. That said, it does not identify this line by minimising the sum of squared residuals. Instead, it identifies the line that maximises the margin (which can be thought of as empty space) between the classes (groups) it separates. The points on the boundary of this margin are referred to as ‘support vectors’. support vector machines can also be designed to allow a small number of data points to be misclassified. Although counter-intuitive, allowing errors can make one’s model more generalisable (Pisner and Schnyer 2020).\n\n\n\nAn example of a support vector margin separating data into two classifications - MathWorks (n.d.)"
  },
  {
    "objectID": "wk7.html#applications-in-research",
    "href": "wk7.html#applications-in-research",
    "title": "7  Classification of Remotely Sensed Data",
    "section": "7.2 Applications in Research",
    "text": "7.2 Applications in Research\nThe importance of classification in remote sensing makes it a near ubiquitous phenomena in papers using remote sensing methods. With that said, I’d like to focus on one paper that makes specific use of random forests for classification, and another which deploys support vector machines.\nUnlike Hansen et al. (2013), Linhui, Weipeng, and Huihui (2021) make use of random forests not to identify changes in forest cover but to classify forests based on their type. In order to classify forest cover in the Maoershan Nature Reserve (Heilonjiang Province, China), they used imagery from the Gaofen-2 satellite which, after being atmospherically corrected, was fed into a random forest program with 650 trees, with no pruning applied. Six forest types were identified which included ‘Scotch Pine Forest’, ‘Oak Forest’, and ‘Mixed Wood’, to name a few. Interestingly, Linhui, Weipeng, and Huihui (2021) also applied a support vector machine classifier to the same dataset, which yielded 79.86% predictive accuracy as opposed to the 83.16% accuracy reached by the random forest method.\nIn comparison, Halder, Das, and Basu (2022) use a support vector machine as the primary method of generating a land use and land cover (LULC) classifier to investigate the environmental impact of irrigation products in the Shali reservoir area of Eastern India. They generate six LULC maps ranging from 1995 to 2020 derived from the Landsat platform, before applying cellular automata methods to predict future land use change in the area based on the results of those classifications. In addition to traditional validation methods, Halder, Das, and Basu (2022) also use population, agricultural data, and other non-remotely sensed datasets to validate the classifications created through their SVM classifier."
  },
  {
    "objectID": "wk7.html#reflection",
    "href": "wk7.html#reflection",
    "title": "7  Classification of Remotely Sensed Data",
    "section": "7.3 Reflection",
    "text": "7.3 Reflection\nMy previous experience with classification to this point has always been with products like the National Land Cover Database from the USGS, where pixels have already been classified for use. To that end, the actual act of classification has always been a bit of a black box for me. Although it’s perhaps unsurprising, I was surprised to the extent to which the methods commonly deployed are nearly all based in what I think of as machine learning methods. I had expected methods which focused more specifically on the attributes of pixels in each band a sensor might capture as opposed to just segmenting our dataset as ‘purely’ as possible. To that end, I also thought that our classification categories were always preselected instead of identifying natural breaks between potential categories without human input.\nWhile it makes sense in retrospect that ML would be the driving force behind modern classification methods, I do think the point raised in lecture about the explainability of certain methods is a particularly pertinent one. The fact that the application of these classification algorithms could massively overcomplicate what may, depending on the application, be a really simple problem is something that we, as spatial data scientists, need to be aware of. I’m definitely familiar with the desire to simply throw the ‘best’ or ‘most accurate’ tool at a problem even if it’s a bit inappropriate for the application at hand. This is especially true when systems like Earth Engine allow you to select your model and begin training in one or two lines of code. Remembering what it is these algorithms are actually doing, then, will remain crucial.\nFinally, the experience of actually performing classification emphasised to me the importance of identifying accurate training data. When selecting parts of my image which I sought to classify to train my model, I was pretty ‘coarse’ in my selection of data and it was immediately obvious to see that I hadn’t been careful enough in capturing my classification types - my training data was all captured perfectly but everything else became a bit of a mess, implying that my model may be over fit on my training dataset."
  },
  {
    "objectID": "wk7.html#references",
    "href": "wk7.html#references",
    "title": "7  Classification of Remotely Sensed Data",
    "section": "7.4 References",
    "text": "7.4 References\n\n\n\n\nBiau, Gérard, and Erwan Scornet. 2016. “A Random Forest Guided Tour.” TEST 25 (2): 197–227. https://doi.org/10.1007/s11749-016-0481-7.\n\n\nHalder, Subhra, Subhasish Das, and Snehamanju Basu. 2022. “Use of Support Vector Machine and Cellular Automata Methods to Evaluate Impact of Irrigation Project on LULC.” Environmental Monitoring and Assessment 195 (1): 3. https://doi.org/10.1007/s10661-022-10588-6.\n\n\nHansen, M. C., P. V. Potapov, R. Moore, M. Hancher, S. A. Turubanova, A. Tyukavina, D. Thau, et al. 2013. “High-Resolution Global Maps of 21st-Century Forest Cover Change.” Science 342 (6160): 850–53. https://doi.org/10.1126/science.1244693.\n\n\nIBM. n.d. “What Is a Decision Tree  IBM.” Accessed February 27, 2024. https://www.ibm.com/topics/decision-trees.\n\n\nLaber, Eduardo, and Lucas Murtinho. 2019. “Minimization of Gini Impurity: NP-Completeness and Approximation Algorithm via Connections with the k-Means Problem.” Electronic Notes in Theoretical Computer Science, The proceedings of lagos 2019, the tenth latin and american algorithms, graphs and optimization symposium (LAGOS 2019), 346 (August): 567–76. https://doi.org/10.1016/j.entcs.2019.08.050.\n\n\nLinhui, Li, Jing Weipeng, and Wang Huihui. 2021. “Extracting the Forest Type from Remote Sensing Images by Random Forest.” IEEE Sensors Journal 21 (16): 17447–54. https://doi.org/10.1109/JSEN.2020.3045501.\n\n\nLoh, Wei-Yin. 2011. “Classification and Regression Trees.” WIREs Data Mining and Knowledge Discovery 1 (1): 14–23. https://doi.org/10.1002/widm.8.\n\n\nMathWorks. n.d. “Support Vector Machine (SVM).” https://www.mathworks.com/discovery/support-vector-machine.html.\n\n\nPisner, Derek A., and David M. Schnyer. 2020. “Chapter 6 - Support Vector Machine.” In, edited by Andrea Mechelli and Sandra Vieira, 101–21. Academic Press. https://doi.org/10.1016/B978-0-12-815739-8.00006-7.\n\n\nShafi, Adam. 2023. “Random Forest Classification with Scikit-Learn.” https://www.datacamp.com/tutorial/random-forests-classifier-python."
  },
  {
    "objectID": "wk8.html#summary",
    "href": "wk8.html#summary",
    "title": "8  Advanced Classification and Accuracy Assesment",
    "section": "8.1 Summary",
    "text": "8.1 Summary\nA continuation of last week’s look into the classification of remotely sensed imaging, we focused this week on a handful of more advanced methods, with a particular focus on determining the accuracy of classifications. I will briefly touch on some of those methods before taking the majority of the summary to talk about accuracy assessment.\n\n8.1.1 Object Based Image Analysis\nObject based image analysis is a method of analysing remotely sensed data at the multi-pixel or ‘superpixel’ level by identifying discrete physical objects (buildings, small bodies of water, fields, etc…) which can then be classified at object level as opposed to considering pixels in isolation (Blaschke 2010).\n\n\n\nA Remotely Sensed Image segmented using OBIA - GISGeography (2023)\n\n\nThe most common way of doing this is an algorithm known as Simple Linear Iterative Clustering, which partitions the image into a grid before iteratively moving the centre of each cell to best capture the pixels which maximise the homogeneity of the chunk, in a manner similar to k-means clustering (Achanta et al. 2012). This algorithm attempts to keep objects both spatially and spectrally compact, meaning they retain a consistent appearance and a small physical distance from end to end.\n\n\n8.1.2 Sub-Pixel Analysis\nIn contrast to analysis at the multi-pixel analysis, sub-pixel analysis concerns the classification of individual pixels when they may fall into multiple land-cover categories. This is particularly important for satellite imagery taken at a low spatial resolution, where individual pixels represent a larger share of land area (DU et al. 2014). A classic method for sub-pixel analysis is multiple endmember spectral mixture analysis, also known as MESMA, which attempts to identify the proportion of each landcover type within each pixel. To do this, each pixel’s spectral signature is compared against endmembers (spectral representations of landcover types - often but not always soil, vegetation, and impervious surfaces) to identify the endmembers which most contributed to the pixel’s ‘mixed signature’ (Quintano, Fernández-Manso, and Roberts 2013). This process is known as unmixing - basically breaking a pixel down into its constituent landcover components.\n\n\n8.1.3 Accuracy Assessment\nThe key issue we focused on in lecture (and which I would like to focus on in this review) is that of determining how accurately a landcover classification algorithm has actually divided an image. There are two key accuracy metrics to consider:\n\nProducer’s Accuracy\nProducer’s accuracy, or PA, is the percentage of pixels correctly classified by landcover (Barsi et al. 2018). In other words, if I select a pixel that’s ‘bare earth’, what’s the likelihood that it has been classified as bare earth by our algorithm?\nUser’s Accuracy\nUser’s Accuracy is a measure of the proportion of pixels classified as a particular class which actually belong to that class (Barsi et al. 2018). In other words, if I select a random pixel which was classified as ‘bare earth’, what’s the likelihood that it’s actually bare earth?\n\nThere is an inherent trade-off between these two metrics - if an algorithm is ‘quick to classify’ pixels as a certain class when they may not belong to that class, it ensures that all pixels of that class are captured but the algorithm may also capture pixels not belonging to that class. On the other hand, an algorithm which only classifies pixels as a given class when it’s absolutely positive is one for which you can assume those pixels to be of that class with a high degree of confidence, but there will likely be many pixels of that class not captured by the algorithm.\nThere are a handful of metrics which attempt to capture, in a single statistic, the accuracy of a classification algorithm. One of the most widely used, the Kappa coefficient, attempts to measure how well a model classifies an image when compared to an algorithm randomly classifying pixels (Foody 2020). There are, however, a number of issues with the measure with some arguing that it is difficult to interpret and doesn’t accurately capture ‘chance agreement’ as intended (Foody 2020).\n\n\n8.1.4 Machine Learning Approaches\nFinally, we touched on a handful of approaches which deploy machine learning to in order to maximise classification accuracy. The basic principle underlying these principles is that a model trained on a particular dataset will then be tested against a portion of that dataset which it has not yet seen. Typically this means that a dataset will be subdivided into folds, where each fold can be used as a test set for an algorithm trained on all other folds (Stock and Subramaniam 2022). One version of this approach, known as ‘leave-one-out cross-validation’, uses a number of folds equivalent to the number of observations in the dataset, effectively testing each algorithm on a single data point (Stock and Subramaniam 2022).\nA key issue, unique to spatial data, arises in the form of spatial autocorrelation. If our testing and training data are in close proximity, they’re likely to be similar to each other. The test set is unlikely, then, to represent a novel sort of data which can accurately assess the algorithm at hand. A solution comes in the form of random k-fold cross-validation, where the data is not split randomly but spatially, and individual folds are spatially non-overlapping (Wang, Zhodadadzadeh, and Zurita-Milla 2023). More advanced methods will further subdivide these folds into inner folds and test multiple models with randomly selected parameters in attempt to find the best possible model."
  },
  {
    "objectID": "wk8.html#applications-in-research",
    "href": "wk8.html#applications-in-research",
    "title": "8  Advanced Classification and Accuracy Assesment",
    "section": "8.2 Applications in Research",
    "text": "8.2 Applications in Research\nFor this section I will discuss both a paper that expands on the methods described above as well as one which applies these methods for a particular use case.\nStock and Subramaniam (2022) propose a novel method of leave-one-out cross-validation, which they call iterative spatial leave-one-out cross-validation, or iSLOOCV. Typical spatial leave-one-out cross-validation (SLOOCV) works by excluding any observations within a certain distance \\(r\\) of the single test set variable to avoid spatial autocorrelation effects. The method proposed by Stock and Subramaniam (2022) introduces an iterative element, wherein \\(r\\) is varied from 100 metres to 200 kilometres in order to identify shifts in error over the different threshold differences and identify the minimum separation required to avoid spatial autocorrelation effects.\n\n\n\niSLOOCV applied to a set of observations in the Gulf of Mexico - Stock and Subramaniam (2022)\n\n\nAn application of Object-based image analysis which is of particular interest to me is its use in change detection. Given the nature of change (as seen from above) as often concerned with changes in discrete objects at ground-level, it’s easy to see why object-based methods are a sensible choice. Im, Jensen, and Tullis (2008) develop a new method of change detection attempting to evaluate changes in landcover at the object level. To do this, they normalise data across their two images (before and after) and extract reference points which are classified into five ‘unchanged classes’ (landcover was the same between both images) and three ‘change classes’, representing examples of different types of change. They then applied nearest-neighbour and decision tree classification algorithms to their images in order to generate what they called an ‘object correlation image’. They then compared their results against images generated by an existing methodology (neighbourhood correlation image) which classified local, multi-pixel areas but didn’t use any sort of object detection. Object-based performed the best classification and had higher kappa values than the neighbourhood-based analysis (although as discussed previously, kappa coefficients are not a bulletproof metric).\n\n\n\nChange detection results and corresponding class key from a Las Vegas neighbourhood - Im, Jensen, and Tullis (2008)"
  },
  {
    "objectID": "wk8.html#reflection",
    "href": "wk8.html#reflection",
    "title": "8  Advanced Classification and Accuracy Assesment",
    "section": "8.3 Reflection",
    "text": "8.3 Reflection\nIn my opinion, the most telling lesson amidst the topics discussed on remote sensing classes is that of the Kappa coefficient. It’s a reminder to me that even in an academic setting, where one would hope that accuracy is paramount, established methods can take hold even when reasonable alternatives exist. I was surprised to see in Im, Jensen, and Tullis (2008) that the metrics being used to show accuracy were Producer’s accuracy, User’s accuracy, and the Kappa coefficient. No other accuracy metrics were provided. Additionally, the coefficients generated were all within a narrow, 4% band making it difficult to understand a) how good the classifiers are in general and b) if the new object-based method is really all that much better than existing methods.\nI’ve already encountered the use of some of the more robust accuracy metrics we discussed in class when doing research for our group presentation assessment, such the Receiver Operating Characteristic Curve (the details for which I omitted to save space). To that end, I know preferable metrics can be used with relative ease so the lag in adoption is interesting to see."
  },
  {
    "objectID": "wk8.html#references",
    "href": "wk8.html#references",
    "title": "8  Advanced Classification and Accuracy Assesment",
    "section": "8.4 References",
    "text": "8.4 References\n\n\n\n\nAchanta, Radhakrishna, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Süsstrunk. 2012. “SLIC Superpixels Compared to State-of-the-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–82. https://doi.org/10.1109/TPAMI.2012.120.\n\n\nBarsi, Á, Zs Kugler, I. László, Gy Szabó, and H. M. Abdulmutalib. 2018. “Accuracy Dimensions in Remote Sensing.” The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences XLII-3 (April): 61–67. https://doi.org/10.5194/isprs-archives-XLII-3-61-2018.\n\n\nBlaschke, T. 2010. “Object Based Image Analysis for Remote Sensing.” ISPRS Journal of Photogrammetry and Remote Sensing 65 (1): 2–16. https://doi.org/10.1016/j.isprsjprs.2009.06.004.\n\n\nDU, Peijun, Sicong LIU, Pei LIU, Kun TAN, and Liang CHENG. 2014. “Sub-Pixel Change Detection for Urban Land-Cover Analysis via Multi-Temporal Remote Sensing Images.” Geo-Spatial Information Science 17 (1): 26–38. https://doi.org/10.1080/10095020.2014.889268.\n\n\nFoody, Giles M. 2020. “Explaining the Unsuitability of the Kappa Coefficient in the Assessment and Comparison of the Accuracy of Thematic Maps Obtained by Image Classification.” Remote Sensing of Environment 239 (March): 111630. https://doi.org/10.1016/j.rse.2019.111630.\n\n\nGISGeography. 2023. “OBIA - Object-Based Image Analysis (GEOBIA) - GIS Geography.” https://gisgeography.com/obia-object-based-image-analysis-geobia/.\n\n\nIm, J., J. R. Jensen, and J. A. Tullis. 2008. “Object-Based Change Detection Using Correlation Image Analysis and Image Segmentation.” International Journal of Remote Sensing 29 (2): 399–423. https://doi.org/10.1080/01431160601075582.\n\n\nQuintano, Carmen, Alfonso Fernández-Manso, and Dar A. Roberts. 2013. “Multiple Endmember Spectral Mixture Analysis (MESMA) to Map Burn Severity Levels from Landsat Images in Mediterranean Countries.” Remote Sensing of Environment 136 (September): 76–88. https://doi.org/10.1016/j.rse.2013.04.017.\n\n\nStock, Andy, and Ajit Subramaniam. 2022. “Iterative Spatial Leave-One-Out Cross-Validation and Gap-Filling Based Data Augmentation for Supervised Learning Applications in Marine Remote Sensing.” GIScience & Remote Sensing 59 (1): 1281–1300. https://doi.org/10.1080/15481603.2022.2107113.\n\n\nWang, Yanwen, Mahdi Zhodadadzadeh, and Raúl Zurita-Milla. 2023. “Spatial+: A New Cross-Validation Method to Evaluate Geospatial Machine Learning Models.” International Journal of Applied Earth Observation and Geoinformation 121 (July): 103364. https://doi.org/10.1016/j.jag.2023.103364."
  },
  {
    "objectID": "wk9.html#summary",
    "href": "wk9.html#summary",
    "title": "9  Synthetic Aperture Radar",
    "section": "9.1 Summary",
    "text": "9.1 Summary\n[Text]"
  },
  {
    "objectID": "wk9.html#applications-in-research",
    "href": "wk9.html#applications-in-research",
    "title": "9  Synthetic Aperture Radar",
    "section": "9.2 Applications in Research",
    "text": "9.2 Applications in Research\n[Text]"
  },
  {
    "objectID": "wk9.html#reflection",
    "href": "wk9.html#reflection",
    "title": "9  Synthetic Aperture Radar",
    "section": "9.3 Reflection",
    "text": "9.3 Reflection\n[Text]"
  }
]